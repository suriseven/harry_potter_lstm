{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "harry_potter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ZVQKPYtriK"
      },
      "source": [
        "# colab 환경에서 설치되지 않은 필수 패키지 설치\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25NnqltnkK7J"
      },
      "source": [
        "# 트레이닝 데이터 로드\n",
        "- 원본 데이터로부터 문장부호를 제거하고 이를 새로운 문장으로 만들어서 test.nlp에 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3KCmOEWul20"
      },
      "source": [
        "import csv\n",
        "from konlpy.tag import Okt\n",
        "from gensim.models import word2vec\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 해리포터 텍스트 파일 4개를 읽어서 합침\n",
        "# Harry Potter 4의 경우 읽는데 문제가 있어서 해당 부분은 무시\n",
        "harry_potter1 = open('''/content/drive/MyDrive/harry_potter/text/J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt''').read()\n",
        "harry_potter2 = open('''/content/drive/MyDrive/harry_potter/text/J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt''').read()\n",
        "harry_potter3 = open('''/content/drive/MyDrive/harry_potter/text/J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt''').read()\n",
        "harry_potter4 = open('''/content/drive/MyDrive/harry_potter/text/J. K. Rowling - Harry Potter 4 - The Goblet of Fire.txt''', errors='ignore').read()\n",
        "\n",
        "harry_potter = harry_potter1 + harry_potter2 + harry_potter3 + harry_potter4\n",
        "\n",
        "texts = harry_potter.splitlines();\n",
        "\n",
        "twitter = Okt()\n",
        "\n",
        "result = []\n",
        "for line in texts:\n",
        "    if len(line) > 0:\n",
        "      #형태소 분석하기, 단어 기본형 사용\n",
        "      malist = twitter.pos(line, norm=True, stem=True)\n",
        "      r = []\n",
        "      for word in malist:\n",
        "          # 문장부호는 제외\n",
        "          if not word[1] in [\"Punctuation\"]:\n",
        "              r.append(word[0])\n",
        "      #형태소 사이에 공백문자를 넣고 양쪽 공백을 지움으로써 하나의 문장을 구성\n",
        "      rl = (\" \".join(r)).strip()\n",
        "      result.append(rl)\n",
        "    # print(rl)\n",
        "\n",
        "#형태소들을 별도의 파일로 저장 합니다.\n",
        "with open(\"test.nlp\",'w', encoding='utf-8') as fp:\n",
        "    fp.write(\"\\n\".join(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B-JI2T9unT0"
      },
      "source": [
        "# 단어분포 시각화\n",
        "- wordcloud 사용\n",
        "- twitter(Okt) 라이브러리 사용시 태그정보에 명사, 형용사 정보가 나오지 않고 문장부호만 구분 가능하여 문장부호만 제거하여 시각화\n",
        "- 대명사, 조동사등을 제외하면 사람이름이 주로 나오는 것을 확인할 수 있음\n",
        "- 테스트 문장 생성시 사람이름을 주로 사용하여 문장 생성을 진행해봄"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSXglUtZs7tG"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "twitter = Okt()\n",
        "\n",
        "# twitter함수를 통해 읽어들인 내용의 형태소를 분석한다.\n",
        "sentences_tag = []\n",
        "sentences_tag = twitter.pos(harry_potter) \n",
        "\n",
        "noun_adj_list = []\n",
        "\n",
        "\n",
        "# tag가 명사이거나 형용사인 단어들만 noun_adj_list에 넣어준다.\n",
        "for word, tag in sentences_tag:\n",
        "    if tag not in ['Punctuation']: \n",
        "        noun_adj_list.append(word)\n",
        "\n",
        "\n",
        "# 가장 많이 나온 단어부터 100개를 저장한다.\n",
        "counts = Counter(noun_adj_list)\n",
        "tags = counts.most_common(100) \n",
        "\n",
        "print(tags)\n",
        "# WordCloud를 생성한다.\n",
        "# 한글이 없어서 별도로 나눔고딕을 사용하지는 않음\n",
        "wc = WordCloud(background_color=\"white\", max_font_size=60)\n",
        "cloud = wc.generate_from_frequencies(dict(tags))\n",
        "\n",
        "# 생성된 WordCloud를 test.jpg로 보낸다.\n",
        "\n",
        "cloud.to_file('test.jpg')\n",
        "plt.figure(figsize=(30, 20))\n",
        "plt.axis('off')\n",
        "plt.imshow(cloud)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GoNWWwCiKss"
      },
      "source": [
        "# word2vec 임베딩 벡터 산출\n",
        "- https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b 으로부터 트레이닝 소스 받아서 사용\n",
        "- 과제2에 사용한 데이터에 비해 자료양이 크므로 임베딩 벡터 사이즈를 500으로 늘리고 윈도우 사이즈로 15로 늘림\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSWUPM6fLnnY"
      },
      "source": [
        "################################################################################\n",
        "# folked from https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b  #\n",
        "################################################################################\n",
        "\n",
        "import numpy as np\n",
        "import gensim\n",
        "import string\n",
        "\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils.data_utils import get_file\n",
        "\n",
        "print('\\nPreparing the sentences...')\n",
        "\n",
        "# 트레이닝 벡터의 사이즈에 따라 최대 문장 길이를 제한하고 대문자는 모두 소문자로 치환하여 word2vec 데이터 구성\n",
        "max_sentence_len = 40\n",
        "wData = word2vec.LineSentence(\"test.nlp\")\n",
        "sentences = [[word.lower() for word in w[:max_sentence_len] ] for w in wData] \n",
        "\n",
        "print('Num sentences:', len(sentences))\n",
        "\n",
        "print('\\nTraining word2vec...')\n",
        "word_model = gensim.models.Word2Vec(sentences, size=500, min_count=1, window=15, iter=100)\n",
        "pretrained_weights = word_model.wv.vectors\n",
        "vocab_size, emdedding_size = pretrained_weights.shape\n",
        "print('Result embedding shape:', pretrained_weights.shape)\n",
        "print('Checking similar words:')\n",
        "\n",
        "# 해리포터 등장인물에 대해 유사단어 검색\n",
        "for word in ['vernon', 'harry', 'weasley', 'hermione']:\n",
        "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n",
        "  print('  %s -> %s' % (word, most_similar))\n",
        "\n",
        "# 모델 저장\n",
        "word_model.save(\"/content/drive/MyDrive/harry_potter/save_model/word2vec.model\")\n",
        "print(\"\\Word2Vec Modeling finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AdEG-fshza6"
      },
      "source": [
        "# LSTM 트레이닝\n",
        "- https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b 으로부터 트레이닝 소스 받아서 사용\n",
        "- 매 epoch마다 테스트를 진행하는데 이 때 해리포터4의 마지막 문단에 나오는 문장들로 테스트를 진행하여 실제 원본 텍스트와 얼마나 차이나는지 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5qiLmfBBnIP"
      },
      "source": [
        "################################################################################\n",
        "# folked from https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b  #\n",
        "################################################################################\n",
        "\n",
        "\n",
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]\n",
        "\n",
        "print('\\nPreparing the data for LSTM...')\n",
        "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
        "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, word in enumerate(sentence[:-1]):\n",
        "    train_x[i, t] = word2idx(word)\n",
        "  train_y[i] = word2idx(sentence[-1])\n",
        "print('train_x shape:', train_x.shape)\n",
        "print('train_y shape:', train_y.shape)\n",
        "\n",
        "print('\\nTraining LSTM...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
        "model.add(LSTM(units=emdedding_size))\n",
        "model.add(Dense(units=vocab_size))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "  if temperature <= 0:\n",
        "    return np.argmax(preds)\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "def generate_next(text, num_generated=10):\n",
        "  word_idxs = [word2idx(word) for word in text.lower().split()]\n",
        "  for i in range(num_generated):\n",
        "    prediction = model.predict(x=np.array(word_idxs))\n",
        "    idx = sample(prediction[-1], temperature=0.7)\n",
        "    word_idxs.append(idx)\n",
        "  return ' '.join(idx2word(idx) for idx in word_idxs)\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "  print('\\nGenerating text after epoch: %d' % epoch)\n",
        "  texts = [\n",
        "    'Uncle Vernon was waiting',\n",
        "    'She hugged Harry',\n",
        "    'Harry winked at them',\n",
        "    'There was no point',\n",
        "  ]\n",
        "  for text in texts:\n",
        "    sample = generate_next(text)\n",
        "    print('%s... -> %s' % (text, sample))\n",
        "\n",
        "model.fit(train_x, train_y,\n",
        "          batch_size=128,\n",
        "          epochs=100,\n",
        "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])\n",
        "\n",
        "model.save(\"/content/drive/MyDrive/harry_potter/save_model/test.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_323158iSKd"
      },
      "source": [
        "# 테스트\n",
        "- 저장해둔 word2vec 임베딩 벡터와 LSTM 모델을 불러옴\n",
        "- generate_next() 함수안에 문장을 넣어 호출하면 이어지는 텍스트를 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2MOWwVoAqWq"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "word_model = word2vec.Word2Vec.load(\"/content/drive/MyDrive/harry_potter/save_model/word2vec.model\")\n",
        "model = load_model(\"/content/drive/MyDrive/harry_potter/save_model/test.model\")\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "  if temperature <= 0:\n",
        "    return np.argmax(preds)\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "\n",
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]\n",
        "\n",
        "def generate_next(text, num_generated=10):\n",
        "  word_idxs = [word2idx(word) for word in text.lower().split()]\n",
        "  for i in range(num_generated):\n",
        "    prediction = model.predict(x=np.array(word_idxs))\n",
        "    idx = sample(prediction[-1], temperature=0.7)\n",
        "    word_idxs.append(idx)\n",
        "  return ' '.join(idx2word(idx) for idx in word_idxs)\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "  print('\\nGenerating text after epoch: %d' % epoch)\n",
        "  texts = [\n",
        "    'Uncle Vernon was waiting',\n",
        "    'She hugged Harry',\n",
        "    'Harry winked at them',\n",
        "    'There was no point',\n",
        "  ]\n",
        "  for text in texts:\n",
        "    sample = generate_next(text)\n",
        "    print('%s... -> %s' % (text, sample))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqFbYLI5IGWq"
      },
      "source": [
        "# 테스트\n",
        "print(generate_next(\"Uncle Vernon was waiting\"))\n",
        "print(generate_next(\"She hugged Harry\"))\n",
        "print(generate_next(\"Harry winked at them\"))\n",
        "print(generate_next(\"There was no point\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
